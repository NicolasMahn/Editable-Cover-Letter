Your mission to make natural, reliable voice agents effortless for enterprise teams aligns with how I work: applied LLM engineering anchored in rigorous evaluation and clean, production-minded Python. I enjoy turning real conversational data into better prompts, tools, and services. Recently, I built an LLM agent for interactive data analysis, an experimental RAG store, and prompt‑engineering learning materials—each refined via logs, structured experiments, and clear success metrics.

I’ve designed system/meta‑prompts and tool‑use scaffolding, then iterated using conversation and error traces to improve retrieval fidelity and task completion. To prevent regressions, I built typed, pytest‑driven harnesses to compare prompt/model variants, run A/B tests, and instrument runs for traceability. I’ve integrated OpenAI and Anthropic APIs for agentic workflows and developed small Python services (FastAPI) with strong data hygiene and PII practices—habits reinforced at KPMG and Deutsche Bahn, where compliance and auditability were essential.

While my agent work has been text‑first, the same techniques—latency‑aware evaluation, grounding and repair strategies, and tool routing—map directly to voice. I’m eager to collaborate with your customers to translate feedback into deployed improvements and to connect conversational quality to business outcomes through robust evaluation pipelines.