Die Databricks Masterclass bei Deloitte verbindet genau das, was ich suche: ein strukturierter, praxisnaher Ramp‑up zum Lakehouse‑ und Databricks‑Spezialisten mit direktem Übergang in Kundenprojekte – in einem Umfeld, das Exzellenz, Teamgeist und nachhaltigen Impact vereint. Ich möchte meine Erfahrungen in Data Engineering und ML gezielt in skalierbare Lakehouse‑Architekturen auf AWS/Azure/GCP übertragen, um Kundenteams von der Use‑Case‑Definition über Datenintegration und Governance bis in den produktiven Betrieb zu begleiten.

Fachlich bringe ich eine belastbare End‑to‑End‑Basis mit: Im HFU‑KISS‑Kontext habe ich ein experimentelles RAG‑Setup mit Vektordatenbank, Prompt‑Strategien und Evaluationskonzepten aufgebaut und Wissen in Workshops vermittelt. In der MIR‑Forschung habe ich eine geospatiale ML‑Pipeline zur automatisierten Radweg‑Erkennung entwickelt (Springer‑Publikation) – von Datenaufbereitung über Feature‑Pipelines bis zur Evaluation nach CRISP‑ML(Q). Praktisch ergänzt wird dies durch Stationen bei KPMG und der Deutschen Bahn an der Schnittstelle Business–Tech (Datenintegration, Reporting/Analytics). Technisch arbeite ich sicher mit Python/SQL, ersten PySpark‑Workloads, ETL/ELT, Datenqualität und Governance sowie MLOps‑Grundlagen (Git, DVC, CI/CD, Container) und IaC mit Terraform. Deutsch und Englisch beherrsche ich sicher, agile Zusammenarbeit ist mir vertraut.

Ich freue mich darauf, nach der fünften Woche in multidisziplinären Teams moderne Datenplattformen mit Databricks, PySpark und Terraform voranzutreiben und messbaren Nutzen zu liefern. Geplanter Start ist der 01.02.2026, bevorzugt in Stuttgart; Reisebereitschaft und schnelle Zertifizierungen (Databricks/Terraform) bringe ich mit.