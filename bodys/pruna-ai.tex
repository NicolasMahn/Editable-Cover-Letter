Pruna’s focus on making state-of-the-art models efficient, accessible, and sustainable resonates with how I approach applied ML: rigorous benchmarking, pragmatic engineering, and tight feedback loops with users. I’m motivated by turning promising research into dependable, cost‑aware deployments—so teams can adopt advanced models without prohibitive latency, spend, or carbon costs.

In my double M.Sc. and research roles, I built an experimental RAG database, developed an LLM‑Agent for interactive data analysis (thesis), and co‑authored peer‑reviewed work, including a Springer 2023 paper on multi‑stage geospatial ML and a CEUR 2025 paper on VotingAid. Across these projects, I led end‑to‑end experimentation: establishing baselines, benchmarking accuracy/latency/cost trade‑offs, profiling bottlenecks, and translating results into clear, stakeholder‑friendly reports. I work primarily in Python/PyTorch with an MLOps mindset—versioned data/models, containerized services (Docker), reproducible pipelines, and monitoring considerations—and I’m familiar with serving stacks and clouds (vLLM/Litserve/Cog concepts; AWS/Azure/GCP; Modal/Runpod). I understand and apply efficiency methods (quantization/pruning/distillation concepts), make hardware‑aware choices, and am eager to deepen lower‑level skills (Triton/CUDA). Internships at KPMG and Deutsche Bahn strengthened my customer‑facing collaboration and deployment discipline.

I’d be glad to help Pruna analyze new open‑source models, optimize and benchmark them against baselines, package robust deployments for your SaaS platform, and iterate closely with customers. Based in Germany, I’m open to a hybrid setup in Munich and excited to learn relentlessly while contributing to Pruna’s mission.