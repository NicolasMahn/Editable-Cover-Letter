

DeepL’s mission to break language barriers resonates with me, and the Data Foundation team’s focus on turning customer, revenue, and usage signals into trustworthy, reusable data products matches how I work. I bring an engineering mindset with product empathy, comfortable across on‑prem and cloud, and motivated by clear contracts, documentation, and performance.

At KISS, I built an end‑to‑end RAG data product—ingestion, chunking, embeddings, evaluation—designing vector/metadata schemas that supported efficient retrieval and analytics. I backed it with tests, CI, and tutorials for reuse (CRISP‑ML(Q)). At MIR, I turned multi‑source geospatial data into analytics‑ready models with reproducible preprocessing and tuned SQL. Earlier at Deutsche Bahn I automated ingestion in hybrid/cloud setups and improved deployment reliability. Across these projects, I collaborated directly with analysts and scientists, optimized queries, and treated data quality and lineage as first‑class concerns.

At DeepL I aim to deliver stable, testable pipelines and foundational models, implement guardrails for freshness, completeness, and lineage, and help peers optimize queries. I’m prepared to contribute to on‑call rotations and post‑mortems. To ramp quickly, I’m planning to experiment with a dbt + Airflow setup on Snowflake/Databricks/BigQuery, using data contracts and Great Expectations/OpenLineage as I learn to align with your DataOps practices.