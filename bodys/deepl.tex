DeepL’s mission to break language barriers resonates with me, and the Data Foundation team’s focus on turning customer, revenue, and usage signals into trustworthy, reusable data products matches how I work. I bring an engineering mindset with product empathy, comfortable across on‑prem and cloud, and motivated by clear contracts, documentation, and performance.

At KISS, I built an end‑to‑end RAG data product—ingestion, chunking, embeddings, evaluation—designing vector/metadata schemas that supported efficient retrieval and analytics. I backed it with tests, CI, and tutorials for reuse (CRISP‑ML(Q)). At MIR, I turned multi‑source geospatial data into analytics‑ready models with reproducible preprocessing and tuned SQL. Earlier at Deutsche Bahn I automated ingestion in hybrid/on‑prem setups and improved deployment reliability; at KPMG I shipped documented SQL/Python transformations with consistent definitions. Across these projects, I collaborated directly with analysts and scientists, optimized queries, and treated data quality and lineage as first‑class concerns.

At DeepL I aim to deliver stable, testable pipelines and foundational models, implement guardrails for freshness, completeness, and lineage, and help peers optimize queries. I’m prepared to contribute to on‑call rotations and post‑mortems. To ramp quickly, I plan to stand up a dbt + Airflow stack on Snowflake/Databricks/BigQuery with data contracts and Great Expectations/OpenLineage, aligning with your DataOps standards.