Black Forest Labs’ trajectory—from Latent Diffusion and Stable Diffusion to FLUX.1—resonates with my belief that model quality is ultimately a data problem: fast, transparent, and ethically sourced pipelines create the leverage for breakthroughs. I’m motivated to help you scale licensed image/video acquisition and turn heterogeneous, billion-scale corpora into training-ready assets with high diversity and strong observability.

In a geospatial computer vision project (Springer 2023), I built preprocessing pipelines for massive imagery: OpenCV/FFmpeg for conversion, sampling, tiling, and metadata extraction; Python (pandas/NumPy/PyTorch) for batch transforms; and quality gates to keep data consistently trainable. I’ve applied embedding-based similarity search to cluster, visualize, and deduplicate datasets, exposing issues early and enabling targeted curation. For enrichment, I prototyped LLM-driven annotation/captioning flows in my RAG/agent work (KISS), with governance and eval signals guiding iteration. I prioritize reproducibility and performance through containerized workflows, DVC-style data/versioning, and orchestration patterns (Airflow/Flyte/Kubeflow concepts), with practical use of CPUs/GPUs and basic Slurm/HPC familiarity. From internships at KPMG and Deutsche Bahn, I bring habits for secure, auditable data transfers across filesystems/object stores, including integrity checks via metadata and hashing.

I’m eager to work in the model–data loop, updating datasets as training reveals gaps, and to build tools that make your data distributions legible—clustering, visualization, and scalable processing that respect licensing and ethics. Based in Germany, I’m ready to contribute on-site in Freiburg and keep learning alongside your team.